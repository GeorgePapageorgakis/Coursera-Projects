/**
    MP5
    list_reduction.cpp
    Purpose: MP Reduction. Given a list (lst) of length n Output its sum = lst[0] + lst[1] + ... + lst[n-1];
    Implement a kernel the performs reduction of a 1D list. The reduction should give the sum of the list. 
    Improved kernel should be able to handle input lists of arbitrary length. However, for simplicity, 
    you can assume that the input list will be at most 2048 x 65535 elements so that it can be handled 
    by only one kernel launch. The boundary condition can be handled by filling “identity value (0 for sum)”
    into the shared memory of the last block when the length is not a multiple of the thread block size. 
    Further assume that the reduction sums of each section generated by individual blocks will be summed
    up by the CPU.
    
    @author George Papageorgakis
    @version 1.0 03/2015
*/
#include    <wb.h>

#define BLOCK_SIZE 256 //@@ You can change this

#define wbCheck(stmt) do {                                                    \
        cudaError_t err = stmt;                                               \
        if (err != cudaSuccess) {                                             \
            wbLog(ERROR, "Failed to run stmt ", #stmt);                       \
            wbLog(ERROR, "Got CUDA error ...  ", cudaGetErrorString(err));    \
            return -1;                                                        \
        }                                                                     \
    } while(0)

__global__ void reduction_1D_kernel(float * input, float * output, int inlen) {
   	__shared__ float in_s[BLOCK_SIZE];
	unsigned int i, tid = blockIdx.x * blockDim.x + threadIdx.x;
	unsigned int stride = blockDim.x * gridDim.x;

	while (tid < inlen){
		//Load a segment of the input vector into shared memory
		in_s[threadIdx.x] = input[tid];
		__syncthreads();
		//@@ Traverse the reduction tree
		// Parallel Reduction: Sequential Addressing in shared mem 
		// (bitwise right shift i), reversed loop and threadID-based 
		// indexing - shift operations avoid expensive divisions and modulos.
		for (i = blockDim.x/2; i > 0; i >>= 1){
			if (threadIdx.x < i){
				in_s[threadIdx.x] += in_s[threadIdx.x + i];
			}
			__syncthreads();
		}
		// We now have the sum value of each block stored in 
		// in_s[0] and we can store it in output[] global array	
		// Write the computed sum of the block to  
		// the output vector at the correct index
		if (threadIdx.x == 0)
			output[tid/(blockDim.x)] = in_s[0];
		
		__syncthreads();
		tid += stride;
		in_s[threadIdx.x] = 0;
		__syncthreads();
	}
}

int main(int argc, char ** argv) {
    int ii;
    wbArg_t args;
    float * hostInput; // The input 1D list
    float * hostOutput; // The output list
    float * deviceInput;
    float * deviceOutput;
    int numInputElements; // number of elements in the input list
    int numOutputElements; // number of elements in the output list

    args = wbArg_read(argc, argv);

    wbTime_start(Generic, "Importing data and creating memory on host");
    hostInput = (float *) wbImport(wbArg_getInputFile(args, 0), &numInputElements);
	numOutputElements = (numInputElements-1)/(BLOCK_SIZE) + 1;
    //numOutputElements = numInputElements / ((BLOCK_SIZE)<<1);
    //if (numInputElements % ((2*BLOCK_SIZE)<<1)) {
    //    numOutputElements++;
    //}
    hostOutput = (float*) malloc(numOutputElements * sizeof(float));
    wbTime_stop(Generic, "Importing data and creating memory on host");
	
    wbLog(TRACE, "The number of input elements in the input is ", numInputElements);
    wbLog(TRACE, "The number of output elements in the input is ", numOutputElements);

    wbTime_start(GPU, "Allocating GPU memory.");
    //@@ Allocate GPU memory here
	wbCheck(cudaMalloc((void **) &deviceInput, numInputElements * sizeof(float)));
	wbCheck(cudaMalloc((void **) &deviceOutput, numOutputElements * sizeof(float)));
    wbTime_stop(GPU, "Allocating GPU memory.");

    wbTime_start(GPU, "Copying input memory to the GPU.");
    //@@ Copy memory to the GPU here
	wbCheck(cudaMemcpy(deviceInput, hostInput, numInputElements * sizeof(float), cudaMemcpyHostToDevice));
    wbTime_stop(GPU, "Copying input memory to the GPU.");

	//@@ Initialize the grid and block dimensions here
	dim3 dimBlock(BLOCK_SIZE, 1, 1);
	dim3 dimGrid(numOutputElements, 1, 1);
    
	wbTime_start(Compute, "Performing CUDA computation");
    //@@ Launch the GPU Kernel here
	reduction_1D_kernel<<<dimGrid, dimBlock>>>(deviceInput, deviceOutput, numInputElements);
    cudaDeviceSynchronize();
    wbTime_stop(Compute, "Performing CUDA computation");

    wbTime_start(Copy, "Copying output memory to the CPU");
    //@@ Copy the GPU memory back to the CPU here
	wbCheck(cudaMemcpy(hostOutput, deviceOutput, numOutputElements * sizeof(float), cudaMemcpyDeviceToHost));
    wbTime_stop(Copy, "Copying output memory to the CPU");

    /********************************************************************
     * Reduce output vector on the host
     * NOTE: One could also perform the reduction of the output vector
     * recursively and support any size input. For simplicity, we do not
     * require that for this lab.
     ********************************************************************/		 
    for (ii = 1; ii < numOutputElements; ii++) {
        hostOutput[0] += hostOutput[ii];
    }

    wbTime_start(GPU, "Freeing GPU Memory");
    //@@ Free the GPU memory here
    cudaFree(deviceInput);
    cudaFree(deviceOutput);
    wbTime_stop(GPU, "Freeing GPU Memory");

    wbSolution(args, hostOutput, 1);

    free(hostInput);
    free(hostOutput);

    return 0;
}
